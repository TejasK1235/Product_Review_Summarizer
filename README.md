# ğŸ§  Product Review Summarizer

### A Natural Language Processing (NLP) Project  
Summarizing multiple customer reviews of digital music products using **Extractive** and **Abstractive** text summarization techniques.

---

## ğŸ“– Project Overview

This project implements an intelligent system that generates concise summaries of product reviews.  
Given a product ID (ASIN), product title, or a block of user-provided text, the system fetches relevant reviews and produces a readable summary highlighting the overall sentiment and key feedback points.

The goal is to help users quickly understand collective opinions about a product without reading hundreds of individual reviews.

---

## ğŸ¯ Objectives

- Develop a **working NLP pipeline** that can summarize product reviews accurately.  
- Compare **Extractive** and **Abstractive** summarization techniques.  
- Build an **interactive Flask web application** for easy use.  
- Evaluate summarization performance using standard metrics (ROUGE-1, ROUGE-2, ROUGE-L, BERTScore).

---

## ğŸ§© Features

âœ… **Dual Summarization Options**
- **Extractive Summary:** Selects the most important sentences directly from reviews using TF-IDF and semantic embeddings.  
- **Abstractive Summary:** Generates human-like summaries using a pretrained **BART** transformer model.

âœ… **Smart Review Search**
- Enter a **Product ID (parent_asin)**, **Product Title**, or paste **custom review text** directly.  
- The app automatically fetches and displays all reviews, titles, ratings, and helpfulness scores.

âœ… **User Interface**
- Clean, responsive Flask web app.  
- Option to select summary type (Extractive / Abstractive).  
- Displays reviews and generated summaries in an easy-to-read layout.  
- Handles GPU memory limits gracefully with clear user messages.

âœ… **Automatic Error Handling**
- If abstractive summarization exceeds GPU memory limits, the app automatically falls back to extractive summarization and notifies the user.

---

## ğŸ§  NLP Techniques Used

| Technique | Purpose | Libraries |
|------------|----------|-----------|
| **TF-IDF Vectorization** | Represent review sentences numerically for extractive scoring | scikit-learn |
| **Cosine Similarity + TextRank** | Rank sentences for extractive summarization | numpy, networkx |
| **Sentence-BERT Embeddings** | Measure semantic similarity between sentences | sentence-transformers |
| **Seq2Seq Transformer (BART)** | Generate fluent abstractive summaries | transformers, torch |
| **Evaluation Metrics** | Compare summary quality | evaluate, bert-score |

---

## ğŸ§® Evaluation Metrics

Summaries were evaluated using standard text summarization metrics:

| Metric | Description |
|---------|--------------|
| **ROUGE-1** | Overlap of individual words between system and reference summaries |
| **ROUGE-2** | Overlap of word pairs (bigrams) |
| **ROUGE-L** | Longest common subsequence (sentence structure similarity) |
| **BERTScore** | Semantic similarity using contextual embeddings |

### âœ´ï¸ Key Observations
- **ROUGE-1 â‰ˆ 0.62, ROUGE-2 â‰ˆ 0.56, ROUGE-L â‰ˆ 0.48** â†’ good content coverage.  
- **BERT F1 â‰ˆ 0.54** â†’ abstractive summaries capture core meaning while rephrasing naturally.  
- Abstractive summaries are ~10-15% shorter and more fluent than extractive ones.  

---

## âš™ï¸ Tech Stack

| Layer | Tools / Libraries |
|-------|-------------------|
| **Backend** | Flask, Python 3.x |
| **NLP** | Transformers (BART), Sentence-BERT, NLTK, scikit-learn |
| **Evaluation** | Hugging Face evaluate, bert-score |
| **Frontend** | HTML5, CSS3, Vanilla JS |
| **Hardware Used** | Ryzen 7 CPU, 16 GB RAM, RTX 3060 GPU |

---

## ğŸ§° Installation & Setup

### 1ï¸âƒ£ Clone the repository
```
git clone https://github.com/your-username/product-review-summarizer.git
cd product-review-summarizer
```

### 2ï¸âƒ£ Create a virtual environment
```
python -m venv env
```
Activate it:
```
env\Scripts\activate
```


### 3ï¸âƒ£ Install dependencies
```
pip install -r requirements.txt
```

### 4ï¸âƒ£ Run the Flask app
```
python app.py
```

Then open your browser and visit: http://127.0.0.1:5000


---

## ğŸ§± Project Structure
```
## ğŸ§± Project Structure

Product Review Summarizer/
â”‚
â”œâ”€â”€ app.py                   
â”‚
â”œâ”€â”€ Data/                           
â”‚   â”œâ”€â”€ grouped_reviews.csv         
â”‚   â””â”€â”€ merged_reviews.csv       
â”‚
â”œâ”€â”€ notebooks/                
â”‚   â”œâ”€â”€ Preprocessing.ipynb          
â”‚   â”œâ”€â”€ Extractive_summary.ipynb   
â”‚   â””â”€â”€ Summarization_module.ipynb   
â”‚
â”œâ”€â”€ templates/                   
â”‚   â”œâ”€â”€ index.html                 
â”‚   â””â”€â”€ result.html            
â”‚
â”œâ”€â”€ static/                        
â”‚   â””â”€â”€ style.css                 
â”‚
â”œâ”€â”€ requirements.txt             
â””â”€â”€ README.md                       

```


## ğŸ§© Dataset Information

The system uses preprocessed **Digital Music** product data containing:
- parent_asin â€” unique product ID  
- product_title â€” name of the product  
- review_text â€” user-written review  
- title â€” review headline  
- rating â€” user rating (1â€“5)  
- helpful_vote â€” number of helpful votes  
- verified_purchase â€” boolean indicating verified buyers  
- average_rating â€” productâ€™s overall average rating  

Each product can have multiple reviews grouped by parent_asin.

---

## ğŸ’¡ How It Works (Simplified Flow)

1. **User Input** â†’ Product ID / Title / Custom Text  
2. **Review Retrieval** â†’ Filter dataset by input  
3. **Summarization**
   - *Extractive:* Rank sentences using embeddings and TextRank.
   - *Abstractive:* Generate human-like summary using BART model.  
4. **Output Display** â†’ Summary + all relevant reviews in an organized table.  
5. **Error Handling** â†’ On GPU memory issue, switch to extractive with a friendly message.

---

## ğŸ§­ Results & Insights

- Extractive summarization offers factual, sentence-level summaries.
- Abstractive summarization produces more natural, paraphrased outputs.
- Hybrid system balances **accuracy** and **readability** effectively.
- Demonstrates how transformer-based models outperform classical extractive methods for textual coherence.

---

## ğŸš€ Future Improvements

- Fine-tune the abstractive model on a larger, domain-specific dataset.  
- Add **sentiment-based pros/cons extraction** alongside summaries.  
- Support multilingual product reviews.  
- Add caching to avoid recomputation for the same product IDs.

---


